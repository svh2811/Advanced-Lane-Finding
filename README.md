# Advanced-Lane-Finding

---

Aim of this project is to write a software pipeline to identify the lane boundaries in a video.

---

The goals / steps of this project are the following:

* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
* Apply a distortion correction to raw images.
* Use color transforms, gradients, etc., to create a thresholded binary image.
* Apply a perspective transform to rectify binary image ("birds-eye view").
* Detect lane pixels and fit to find the lane boundary.
* Determine the curvature of the lane and vehicle position with respect to center.
* Warp the detected lane boundaries back onto the original image.
* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.

---

The images for camera calibration are stored in the folder called `data/camera_calibration`.  The images in `data/test_images` are for testing the pipeline on single frames.

### Camera Calibration

#### 1. Computation of the camera matrix and distortion coefficients with an example of a distortion corrected calibration image.

The code for this step is contained in the file `transformation.py` and function `get_camera_calibration_matrix()`.  

I start by preparing "object points", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  

![Corners Connected](./examples/chessboard.jpg)

I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: 

### Pipeline (single images)

#### 1. An example of a distortion-corrected image.

To demonstrate this step, I will describe how I apply the distortion correction to one of the test images like this one:

![Undistored Highway Image 01](./examples/undistort_output_01.png)
![Undistored Highway Image 02](./examples/undistort_output_02.png)

#### 2. Thresholded binary image creation.

I used a combination of color-spaces and gradient thresholds to generate a binary image (thresholding steps at lines # through # in `Advance lane finding pipeline.ipynb` in function `get_thresholded_image()`).  Here's an example of my output for this step.  (note: this is not actually from one of the test images)

Sequence of Image Processing task in pipeline
    1. Color thresholding using LUV color space
    2. Color thresholding using LAB color space
    3. Color thresholding using HSV color space
    4. Combining horizontal and vertical gradients with magnitude threshold
    5. Median blur gradient image
    6. Combine outputs images of steps 1, 2, 3 and 5 

![Thresholding Pipeline](./examples/thresholding-pipeline.png)

#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

The code for my perspective transform is included in a function called `warp_image()` and `get_perspective_matrices()` both are present in `transformation.py`. The `get_perspective_matrices()` function takes two arrays `src` and `dst` to generate a corresponding transformation matrix. `src` and `dst` are handcrafted to represent an perspective transformation, `src` and `dst` are present in `Advance lane finding pipeline.ipynb`.

```
src = np.float32([ [575, 464], [738, 464], [1005, 694], [290, 694] ])
dst = np.float32([ [180, 0], [540, 0], [540, 720], [180, 720] ])
```

The transformation matrix (which is perspective transform in our case) and an image (which has to be transformed) is passed as an argument to function `warp_image()`.

The example shown above is of an color image however for our problem we use binary-image generated by function `get_thresholded_image()`.  

I verified that my perspective transform was working as expected by drawing the `src` and `dst` points onto a test image and its warped counterpart to verify that the lines appear parallel in the warped image.

![Perspective transform of lane image 01](./examples/warped_lane_image_01.png)
![Perspective transform of lane image 02](./examples/warped_lane_image_02.png)

#### 4.Lane line detection

#### Steps of lane line detection:

* If current frame is first video frame:
  + use Sliding Window method
* else
  + use Search from prior
* Either methods would generate a two sets of cor-ordinates one for left lane and one for right lane, these points are feed to `fit_poly()` which attempts to fit a quadratic polynomial curve for each lane 
* ![Lane curves](./examples/color_fit_lines.jpg)

#### Sliding Window
  1. method `group_lane_pixels_using_sliding_window()` present in file `lane_line.py`
  2. Start by generating histgrom of thresholded-binary-perspective-transformed-image to find two lane centers.
  3. ![Histogram](./examples/histogram.png)
  4. Using calculated lane centers, start a sliding window search from bottom of the image and sliding upwards following white pixels in the image.
  5. ![Sliding window](./examples/sliding-window.png)

#### Search from prior
  1. method `group_lane_pixels_using_prev_frame()` present in file `lane_line.py`
  2. This method uses the lane points detected in the  previous image frame to narrow down the search region to search lane.
  3. ![Sliding prior](./examples/saving-prior.png)

#### 5. Calculating radius of curvature of the lane and the position of the vehicle with respect to center.

method `measure_curvature_real_and_car_distance_from_center()` present in `lane_line.py` is used to calulate car's distance from center and lane's curvature. 

#### 6. Drawing Lane.

`overlay_lane_region()` and `write_lane_data()` function present in file `lane_lines.py` are used to color lane region and write curvature radius and car's distance from center information on video image frame. To improve legibility we calculate curvature radius and car position w.r.t. lane center every `3` frames.  

![Sliding prior](./examples/lane-drawn.png)

---

### Pipeline

#### 1. Summary of entire image processing pipeline
![Thresholding Pipeline](./examples/thresholding-pipeline.png)
![Lane Detection Pipeline](./examples/lane-pipeline.png)

#### 2. Video
Here's a [link to my video result](./output/output_videos/project_video.mp4)

---

### Future work

01. Lane region detection pipeline is robust to light intensity change, however the alogirthm might not perform satisfactorily when overall light intensity is poor and thus lane line marking might not be detected by our handcrafted algorithm.  
02. When car is taking a sharp turn one of the lane line markers might not be captured by dashboard camera as that lane marker might me outside of viewing angle, this algorithm might not be able to handle this scenario.
03. History of lane detections is important while determining lane region for current frame. We could encode this fact using a higher capacity sequential model like RNN.
04. Image processing pipeline is slow to use used in realtime.
